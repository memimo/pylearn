The files in this directory recreate some of the experiments reported in the
paper

Maxout Networks. Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron
Courville, and Yoshua Bengio. arXiv 2013

If you use the code provided here, please cite that paper.

The experiments reproduced here are as follows:

1) MNIST permutation invariant result

The paper reports obtaining a test set error rate of 0.94%, the best ever
reported (as of this writing) for a multilayer perceptron with no unsupervised
pretraining. To reproduce this result, run

train.py mnist_pi.yaml
train.py mnist_pi_continued.yaml
print_monitor.py mnist_pi_continued.pkl | grep test_y_misclass

You should get 0.009399999999, i.e., 94 mistakes on the test set.

Note: you probably will want to run the above scripts with
THEANO_FLAGS="device=gpu,floatX=float32" for optimal speed, but if you don't
have a GPU it should work without one. I have, however, not tested this case.
The remaining results all require GPU as they are implemented using Alex Krizvhevsky's
CUDA convolution library.

2) MNIST result

The paper reports obtaining a test set error rate of 0.45%, the state of the
art for the MNIST dataset without data augmentation. To reproduce this result,
run:

THEANO_FLAGS="device=gpu,floatX=float32" train.py mnist.yaml
THEANO_FLAGS="device=gpu,floatX=float32" train.py mnist_continued.yaml
THEANO_FLAGS="device=gpu,floatX=float32" python compute_test_err.py mnist_continued.pkl

The final command should print out some progress messages followed by "0.0045"

3) CIFAR-10 result

The paper reports a test set error of 12.93% on the CIFAR-10 dataset,
which is the state of the art without data augmentation. To reproduce
this result, you must first have prepared a version of the CIFAR-10
dataset with global contrast normalization and whitening. To do so,
run pylearn2/scripts/datasets/make_cifar10_gcn_whitened.py. (If you
are in the LISA lab, don't do this. It has already been made for you)

After running make_cifar10_gcn_whitened, you can train the maxout model
by running

THEANO_FLAGS="device=gpu,floatX=float32" train.py cifar10.yaml

At this point, the model has only been trained on the first 40k examples.
Run
THEANO_FLAGS="device=gpu,floatX=float32" python compute_test_err.py cifar10_best.pkl

It should report a test error of 0.1405 at this point.

In the days ahead, we will provide our yaml config files for training on the
remaining 10k training examples, which reduces the test error to 0.1293.

4) CIFAR-100 result

In the days ahead, we will provide our yaml config files for getting the state of
the art on CIFAR-100.

5) SVHN

In the days ahead, we will provide our yaml config files for getting the state of
the art on SVHN.

